{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-344-gee250c1b Python-3.12.2 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4070 Ti, 12001MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 267 layers, 46108278 parameters, 0 gradients, 107.6 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 모델 로드\n",
    "model = torch.hub.load('.', 'custom', path='my-model/face-detection-yolov5l.pt', source='local')\n",
    "\n",
    "def detect_faces(image, model):\n",
    "    results = model(image)\n",
    "    boxes = []\n",
    "    for *xyxy, conf, cls in results.xyxy[0]:  # x1, y1, x2, y2, confidence, class\n",
    "        if conf > 0.4:  # confidence threshold\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "            boxes.append((x1, y1, x2-x1, y2-y1))\n",
    "    return boxes\n",
    "\n",
    "def blur_faces(image, boxes):\n",
    "    for (x, y, w, h) in boxes:\n",
    "        face = image[y:y+h, x:x+w]\n",
    "        face = cv2.GaussianBlur(face, (99, 99), 30)\n",
    "        image[y:y+h, x:x+w] = face\n",
    "    return image\n",
    "\n",
    "def process_video(input_video_path, output_video_path, model):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, cap.get(cv2.CAP_PROP_FPS), \n",
    "                          (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        boxes = detect_faces(frame, model)\n",
    "        frame = blur_faces(frame, boxes)\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "# 비디오 처리\n",
    "input_video_path = 'input/IMG_6611.MOV'\n",
    "output_video_path = 'output/output8.mp4'\n",
    "process_video(input_video_path, output_video_path, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-344-gee250c1b Python-3.12.2 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4070 Ti, 12001MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46108278 parameters, 0 gradients, 107.6 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video FPS: 59.98624168768631, Original Width: 3840, Original Height: 2160\n",
      "Processed video saved as output/output9.mp4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 모델 로드\n",
    "model = torch.hub.load('.', 'custom', path='my-model/face-detection-yolov5l.pt', source='local')\n",
    "\n",
    "def detect_faces(image, model):\n",
    "    results = model(image)\n",
    "    boxes = []\n",
    "    for *xyxy, conf, cls in results.xyxy[0]:  # x1, y1, x2, y2, confidence, class\n",
    "        if conf > 0.4:  # confidence threshold\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "            boxes.append((x1, y1, x2-x1, y2-y1))\n",
    "    return boxes\n",
    "\n",
    "def blur_faces(image, boxes):\n",
    "    for (x, y, w, h) in boxes:\n",
    "        if x < 0 or y < 0 or x+w > image.shape[1] or y+h > image.shape[0]:\n",
    "            continue\n",
    "        face = image[y:y+h, x:x+w]\n",
    "        if face.size != 0:\n",
    "            face = cv2.GaussianBlur(face, (99, 99), 30)\n",
    "            image[y:y+h, x:x+w] = face\n",
    "    return image\n",
    "\n",
    "def resize_with_aspect_ratio(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
    "    (h, w) = image.shape[:2]\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    if width is not None and height is not None:\n",
    "        ratio = min(width / float(w), height / float(h))\n",
    "        dim = (int(w * ratio), int(h * ratio))\n",
    "    elif width is None:\n",
    "        ratio = height / float(h)\n",
    "        dim = (int(w * ratio), height)\n",
    "    else:\n",
    "        ratio = width / float(w)\n",
    "        dim = (width, int(h * ratio))\n",
    "\n",
    "    resized = cv2.resize(image, dim, interpolation=inter)\n",
    "    return resized\n",
    "\n",
    "def process_video(input_video_path, output_video_path, model, output_width=None, output_height=None):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open input video.\")\n",
    "        return\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"Video FPS: {fps}, Original Width: {original_width}, Original Height: {original_height}\")\n",
    "\n",
    "    if output_width is None and output_height is None:\n",
    "        output_width = original_width\n",
    "        output_height = original_height\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (output_width, output_height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 입력 프레임의 크기 조정 (비율 유지)\n",
    "        frame_resized = resize_with_aspect_ratio(frame, width=output_width, height=output_height)\n",
    "\n",
    "        # 크기 조정된 프레임의 비율을 유지하며 출력 크기로 맞추기 위해 패딩 추가\n",
    "        delta_w = output_width - frame_resized.shape[1]\n",
    "        delta_h = output_height - frame_resized.shape[0]\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "        \n",
    "        color = [0, 0, 0]\n",
    "        frame_padded = cv2.copyMakeBorder(frame_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "\n",
    "        boxes = detect_faces(frame_padded, model)\n",
    "        frame_blurred = blur_faces(frame_padded, boxes)\n",
    "\n",
    "        out.write(frame_blurred)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Processed video saved as {output_video_path}\")\n",
    "\n",
    "# 비디오 처리\n",
    "input_video_path = 'input/IMG_6611.MOV'\n",
    "output_video_path = 'output/output9.mp4'\n",
    "output_width = 640  # 출력 비디오의 너비\n",
    "output_height = 480  # 출력 비디오의 높이\n",
    "\n",
    "process_video(input_video_path, output_video_path, model, output_width, output_height)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
